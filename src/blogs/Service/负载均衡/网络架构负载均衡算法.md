# 负载均衡算法
## 轮询（Round Robin）
依次轮询服务队列的节点列表把每个请求轮流发送到每个服务器上。

一般情况下实现公式如下：
```bash
(当前下标 + 1) % 数据池长度
```
<img src="https://github.com/zygg1512/myBlog/raw/master/images/Service/负载均衡/轮询算法.png" height="300px" />

假设有6次请求分别是1、2、3、4、5、6，两个服务器Server1、Server2。当 请求1 经过负载均衡设备时，按照轮询的算法就是
```bash
(1 + 1) % 2 === 0
(2 + 1) % 2 === 1
(3 + 1) % 2 === 0
(4 + 1) % 2 === 1
(5 + 1) % 2 === 0
(6 + 1) % 2 === 1
```
所以最终 1、3、5 被分发到 Server1 上，而 2、4、6 会被分发到Server2 上

### 缺点
无法考虑服务器的真实情况，该算法比较适合每个服务器的性能差不多的场景，如果有性能存在差异的情况下，那么性能较差的服务器可能无法承担过大的负载（下图的 Server 2）。

<img src="https://github.com/zygg1512/myBlog/raw/master/images/Service/负载均衡/轮询算法缺陷.png" height="300px" />

## 加权轮询（Weighted Round Robbin）
加权轮询是在轮询的基础上，根据服务器的性能差异，为服务器赋予一定的权值，性能高的服务器分配更高的权值。可以动态调整权重并实时同步，是一种相对比较均衡的算法。
例如下图中，服务器 1 被赋予的权值为 5，服务器 2 被赋予的权值为 1，那么 请求1、2、3、4、5 会被发送到 Server1，请求6 会被发送到 Server2

<img src="https://github.com/zygg1512/myBlog/raw/master/images/Service/负载均衡/加权轮询算法.png" height="300px" />

### 缺点

- 权重大的服务节点会突然负载上升
- 由于每个请求的连接时间不一样，使用轮询或者加权轮询算法的话，可能会让一台服务器当前连接数过大，而另一台服务器的连接过小，造成负载不均衡。

例如下图中，请求1、3、5 会被发送到 Server1，但是 请求1、3 很快就断开连接，此时只有 请求5 连接 Server1。请求2、4、6 被发送到 Server2，当 请求2 的连接断开后，此时只有 请求4、6 连接 Server2。该系统继续运行时，Server2 会承担过大的负载。

<img src="https://github.com/zygg1512/myBlog/raw/master/images/Service/负载均衡/加权轮询算法缺陷.png" height="300px" />

## 最少连接（least Connections）
最少连接算法就是将请求发送给当前最少连接数的服务器上。
例如下图中，Server1 当前连接数最小，那么新到来的请求 6 就会被发送到 Server1 上

<img src="https://github.com/zygg1512/myBlog/raw/master/images/Service/负载均衡/最少连接算法.png" height="300px" />

## 加权最少连接（Weighted Least Connection）
在最少连接的基础上，根据服务器的性能为每台服务器分配权重，再根据权重计算出每台服务器能处理的连接数。
**需要实时统计连接数，需要计算权重。**

## 随机算法（Random）
把请求随机发送到服务器上。和轮询算法类似，该算法比较适合服务器性能差不多的场景。

<img src="https://github.com/zygg1512/myBlog/raw/master/images/Service/负载均衡/随机算法.png" height="300px" />


## 源地址哈希法 (IP Hash)
**为了将客户端 IP 请求固定分配给一台服务器，可以让这台服务器存储状态 session；用来实现会话粘滞（Sticky Session）**

使用 IP Hash 的注意点：用户 ip 没有发生更改，并且更改不能把后台服务器直接移除，只能标记 down

### 普通 hash 算法
假如我们有三台缓存服务器编号 Server1、Server2、Server3，现在有 3000 万个key，希望可以将这些 key 均匀的缓存到三台机器上，你会想到什么方案呢？

可能首先想到的方案，是取模算法`hash(key) % N`，对 key 进行 hash 运算后取模，N是机器的数量。

key 进行 hash 后的结果对 3 取模，得到的结果一定是0、1 或者 2，正好对应服务器 Server1、Server2、Server3，存取数据直接找对应的服务器即可，简单粗暴，完全可以解决上述的问题。

<img src="https://github.com/zygg1512/myBlog/raw/master/images/Service/负载均衡/普通hash算法.png" height="300px" />


#### 缺点
取模算法虽然使用简单，但对机器数量取模，在集群扩容和收缩时却有一定的局限性，因为在生产环境中根据业务量的大小，调整服务器数量是常有的事；而服务器数量N发生变化后`hash(key) % N`计算的结果也会随之变化。

<img src="https://github.com/zygg1512/myBlog/raw/master/images/Service/负载均衡/普通hash算法缺陷.png" height="300px" />

比如：一个服务器节点挂了，计算公式从`hash(key)% 3`变成了`hash(key)% 2`，结果会发生变化，此时想要访问一个服务器，这个服务器的位置大概率会发生改变，那么之前缓存的数据也会失去作用与意义。
**大量缓存在同一时间失效，造成缓存的雪崩，进而导致整个缓存系统的不可用**，这基本上是不能接受的，为了解决优化上述情况，一致性 hash 算法应运而生。

### 一致性 hash 算法
一致性hash算法本质上也是一种取模算法，不过，不同于上面按服务器数量取模，一致性hash是对固定值2^32 取模
> IPv4的地址最多有2^32 种，所以用 2^32 可以保证每个IP地址会有唯一的映射

#### hash环
我们可以将这 2^32 个值抽象成一个圆环，圆环正上方的点代表0，顺时针排列，以此类推，1、2、3、4、5、6……直到 2^32 - 1，而这个由2的32次方个点组成的圆环统称为**hash环**
<img src="https://github.com/zygg1512/myBlog/raw/master/images/Service/负载均衡/hash环.png" height="300px" />

那么这个hash环和一致性hash算法又有什么关系？我们还是以上面的场景为例，三台缓存服务器编号node0、node1、node2，3000万个key。

#### 服务器映射到 hash 环
这个时候计算公式就是从`hash(key) % N`变成了`hash(服务器ip) % 2^32`，使用服务器IP地址进行hash计算，用哈希后的结果对 2^32 取模，结果一定是一个 0 到 2^32 - 1 之间的整数，而这个整数映射在hash环上的位置代表了一个服务器，依次将node0、node1、node2三个缓存服务器映射到hash环上。

<img src="https://github.com/zygg1512/myBlog/raw/master/images/Service/负载均衡/服务器映射到hash环.png" height="300px" />


#### 对象key映射到hash环
接着再将需要缓存的key对象也映射到hash环上，`hash(key) % 2^32`，服务器节点和要缓存的key对象都映射到了hash环，那对象key具体应该缓存到哪个服务器上呢？

<img src="https://github.com/zygg1512/myBlog/raw/master/images/Service/负载均衡/对象key映射到hash环.png" height="300px" />

#### 对象key映射到服务器
> **从缓存对象key的位置开始，沿顺时针方向遇到的第一个服务器，便是当前对象将要缓存到的服务器。**

因为被缓存对象与服务器hash后的值是固定的，所以，在服务器不变的条件下，对象key必定会被缓存到固定的服务器上。根据上面的规则，下图中的映射关系：
```bash
key-1 -> node-1
key-3 -> node-2
key-4 -> node-2
key-5 -> node-2
key-2 -> node-0
```
<img src="https://github.com/zygg1512/myBlog/raw/master/images/Service/负载均衡/对象key映射到服务器.png" height="300px" />

如果想要访问某个key，只要使用相同的计算方式，即可得知这个key被缓存在哪个服务器上了

#### 一致性hash的优势
> 我们简单了解了一致性hash的原理，那它又是如何解决集群中添加/缩减节点导致缓存服务大面积不可用的呢？

先来看看扩容的场景，假如业务量激增，系统需要进行扩容增加一台服务器`node-4`，刚好`node-4`被映射到`node-1`和`node-2`之间，沿顺时针方向，发现原本缓存在`node-2`上的对象`key-4`、`key-5`被重新映射到了`node-4`上，而整个扩容过程中受影响的只有`node-4`和`node-1`节点之间的一小部分对象key。

<img src="https://github.com/zygg1512/myBlog/raw/master/images/Service/负载均衡/一致性hash的优势1.png" height="300px" />

反之，假如`node-1`节点宕机，沿顺时针方向缓存在`node-1`上的对象`key-1`被重新映射到了`node-4`上

此时受影响的数据只有`node-0`和`node-1`之间的一小部分对象key。

<img src="https://github.com/zygg1512/myBlog/raw/master/images/Service/负载均衡/一致性hash的优势2.png" height="300px" />

从上边的两种情况发现，当集群中服务器的数量发生改变时，一致性hash算只会影响少部分的对象key，保证了缓存系统整体还可以对外提供服务的。

#### 数据偏斜问题
在服务器节点数量太少的情况下，很容易因为节点分布不均匀而造成**数据倾斜**问题，如下图被缓存的对象大部分缓存在`node-4`服务器上，导致其他节点资源浪费，系统压力大部分集中在`node-4`节点上，这样的集群是非常不健康的。

<img src="https://github.com/zygg1512/myBlog/raw/master/images/Service/负载均衡/数据偏斜问题.png" height="300px" />

解决数据倾斜的办法也简单，我们就要想办法让节点映射到hash环上时，相对分布均匀一点。

一致性Hash算法引入了一个**虚拟节点机制**，即对每个服务器节点计算出多个hash值，它们都会映射到hash环上。**映射到这些虚拟节点上的对象key，最终会缓存在真实的节点上。**

**虚拟节点的hash计算方式**：对应节点的IP地址加数字编号后缀的方式，举个例子
```javascript
//  node-1节点IP为 10.24.23.227，正常计算node-1的hash值
hash(10.24.23.227#1) % 2^32

// 假设给node-1设置三个虚拟节点
// node-1#1、node-1#2、node-1#3，对它们进行hash后取模
hash(10.24.23.227#1) % 2^32
hash(10.24.23.227#2) % 2^32
hash(10.24.23.227#3) % 2^32
```

下图加入虚拟节点后，原有节点在hash环上分布的就相对均匀了，其余节点压力得到了分摊。

<img src="https://github.com/zygg1512/myBlog/raw/master/images/Service/负载均衡/分摊数据偏斜问题1.png" height="300px" />

> 但需要注意一点，分配的虚拟节点个数越多，映射在hash环上才会越趋于均匀，节点太少的话很难看出效果

引入虚拟节点的同时也增加了新的问题，要做虚拟节点和真实节点间的映射，**对象key->虚拟节点->实际节点**之间的转换

<img src="https://github.com/zygg1512/myBlog/raw/master/images/Service/负载均衡/分摊数据偏斜问题2.png" height="300px" />

#### 一致性hash的应用场景
一致性hash在分布式系统中应该是实现负载均衡的首选算法，它的实现比较灵活，既可以在客户端实现，也可以在中间件上实现，比如：

- 日常使用较多的缓存中间件memcached和redis集群
- RPC框架Dubbo用来选择服务提供者
- 分布式关系数据库分库分表：数据与节点的映射关系
- LVS负载均衡调度器

#### 一致性hash的缺点
一致性Hash算法也是有一些潜在隐患的，如果Hash环上的节点数量非常庞大或者更新频繁时，检索性能会比较低下，而且整个分布式缓存需要一个路由服务来做负载均衡，一旦路由服务挂了，整个缓存也就不可用了，还要考虑做高可用。
